# -*- coding: utf-8 -*-
"""Pursuit of Happyness - Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I0NpyvcH-9_OQpY4Ez541QZUNbjwRq_V
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                     # install java
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz     # download spark
!tar xf spark-3.1.1-bin-hadoop3.2.tgz                                                       # untar spark                   
!pip install -q findspark                                                                   # install findspark                                       
!pip install emoji                                                                          # install emoji                               

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"         # set java home        
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"       # set spark home

# mount google drive
from google.colab import drive
drive.mount('/content/drive')

# import required libraries
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# import emoji library
def LDA(preprocessed_posts):
    print(f"Preprocessed {len(preprocessed_posts)} records..")

    # Create a document-term matrix using CountVectorizer
    vectorizer = CountVectorizer()
    doc_term_matrix = vectorizer.fit_transform(preprocessed_posts) # doc_term_matrix is a sparse matrix

    print("LDA Checkpoint 1")
    # Define the number of topics to discover
    num_topics = 10

    # Apply LDA topic modeling
    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42) 
    lda_model.fit(doc_term_matrix) 

    print("LDA Checkpoint 2")
    # Extract the most common topics
    feature_names = vectorizer.get_feature_names_out() 
    most_common_topics = []

    for topic_idx, topic in enumerate(lda_model.components_):   
        top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]    
        most_common_topics.append((topic_idx, top_words))   

    # Print the most common topics
    for topic in most_common_topics:
        print(topic)


# import required libraries
import findspark
findspark.init()
from pyspark.sql import SparkSession
import datetime
from pyspark import AccumulatorParam
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import json

# create spark session
spark = SparkSession.builder \
    .master("local[*]") \
    .getOrCreate()  

# class to accumulate dictionaries
class DictParam(AccumulatorParam):
    def zero(self, value = dict()):
        return value
	
    def addInPlace(self, dict1 ,dict2):
        for k, v in dict2.items():
            if k in dict1:
                dict1[k] = dict1[k] + v
            else:
                dict1[k] = v
        return dict1

# to store the accumulated values
month_acc = spark.sparkContext.accumulator(dict(), DictParam()) #Divide by total length
count_acc = spark.sparkContext.accumulator(dict(), DictParam()) 
word_acc = spark.sparkContext.accumulator(dict(), DictParam())

# map and aggregate
def mapAndAggregate(rec):
    global month_acc
    global count_acc
    rec = rec.split("DELIM")
    try:
        date = datetime.datetime.fromtimestamp(float(rec[1]))
        k = date.month#.strftime("%Y-%m")
        month_acc += { k: float(rec[3]) }
        count_acc += { k: 1 }
    except:
        print("ERROR", rec)
    return rec

# read data
data = spark.sparkContext.textFile("Data/University-Scores/combined-scores").map(mapAndAggregate) 
length = data.count()

acad_year = month_acc.value 
post_count = count_acc.value    
with open("month-acc.json", "w") as fi: 
    json.dump(acad_year, fi)
with open("count-acc.json", "w") as fi:
    json.dump(post_count, fi)

for month in month_acc.value.keys():
    acad_year[month] /= post_count[month]

acad_year = sorted(acad_year.items())
months, values = zip(*acad_year)

# Plot the monthly values
plt.plot(months, values)
plt.xlabel('Year')
plt.ylabel('Value')
plt.title('Monthly Values')
plt.xticks(range(1, 13))  # Set x-axis ticks for each month
# plt.xticks(range(2008, 2024))  # Set x-axis ticks for each month
plt.show()

print("Checkpoint1: ", length)
stressData = data.filter(lambda x: float(x[3]) >= 0.7)
print("Checkpoint2, Stress data: ", stressData.count())

# Topic modeling
TMdata = stressData.map(lambda x: x[2]).collect()
print("Collected stress data, calling LDA")
LDA(TMdata)

# Generate word frequencies
def collect_word_freq(rec):
    global word_acc
    temp = {}
    for word in rec[2].split():
        if word in temp:
            temp[word] += 1
        else:
            temp[word] = 1
    word_acc += temp

# WordCloud
stressData.foreach(collect_word_freq)
print("Word frequencies done")
word_freq = word_acc.value
with open("word-acc.json", "w") as fi:
    json.dump(word_freq, fi)

# # Create the word cloud
# wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

# # Display the word cloud using matplotlib
# plt.figure(figsize=(10, 5))
# plt.imshow(wordcloud, interpolation='bilinear')
# plt.axis('off')
# plt.show()

# stop spark session
spark.stop()

# import required libraries
import json
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords

# Load the English stop words from NLTK
nltk.download('stopwords')
nltk_stop_words = stopwords.words('english')

from stop_words import get_stop_words

# Get the English stop words from the package
py_stop_words = get_stop_words('en')

# Define custom stop words
custom_stop_words = ["anyone", "get", "really", "im", "going", "sure", "y’all", "also", "keep", "one", "like", "else", "i’m", "it’s", "got" , "say", "go" , "come", "dont", "don’t", "take", "day", "thought", "two", "getting", "you’re", "since", "even", "cant", "taken", "know", "someone", "i’ve", "ive", "well", "making", "thing", "didn’t", "haven’t", "want", "yet", "wanted", "make", "already", "coming"]

# Combine the lists of stop words
combined = set(nltk_stop_words + py_stop_words + custom_stop_words)

with open("word-acc.json", "r") as fi:
    word_freq = json.load(fi)

keys_to_delete = [key for key in word_freq.keys() if key in combined or len(key) <= 2]  # Delete stop words and words with length <= 2

for key in keys_to_delete:
    del word_freq[key]

# Create the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()